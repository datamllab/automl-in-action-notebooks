{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/datamllab/automl-in-action-notebooks/master/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# RNN\n",
    "In this section, we will introduce how to use recurrent neural networks for text\n",
    "classification.\n",
    "The dataset we use is the IMDB Movie Reviews.\n",
    "We use the reviews written by the users as the input and try to predict whether the they\n",
    "are positive or negative.\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "You can use the following code to load the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_words = 10000\n",
    "embedding_dim = 32\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=max_words\n",
    ")\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_data[:2])\n",
    "print(train_labels[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The code above would load the reviews into train_data and test_data, load the labels\n",
    "(positive or negative) into train_labels and test_labels. As you can see the reviews in\n",
    "train_data are lists of integers instead of texts. It is because the raw texts cannot be\n",
    "used as an input to a neural network. Neural networks only accepts numerical data as\n",
    "inputs.\n",
    "\n",
    "The integers we see above is the raw text data after a preprocessing step named\n",
    "tokenization. It first split each review into a list of words and assign an integer to\n",
    "each of the words. For example, a scentence \"How are you? How are you doing?\" will be\n",
    "transformed into a list of words as [\"how\", \"are\", \"you\", \"how\", \"are\", \"you\", \"doing\"].\n",
    "Then transformed to [5, 8, 9, 5, 8, 9, 7]. The integers doesn't have special meanings but\n",
    "a representation of the words. Same integers represents the same words, different\n",
    "integers represents different words.\n",
    "\n",
    "The labels are also integers, where 1 represents positive, 0 represents negative.\n",
    "\n",
    "Then, we pad the data to the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Pad the sequence to length max_len.\n",
    "maxlen = 100\n",
    "print(len(train_data[0]))\n",
    "print(len(train_data[1]))\n",
    "train_data = sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=maxlen)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Building your network\n",
    "The next step is to build your neural network model and train it.\n",
    "We will introduce the neural network in three steps.\n",
    "The first step is the embedding, which transform each integer list into a list of vectors.\n",
    "The second step is to feed the vectors to the recurrent neural network.\n",
    "The third step is to use the output of the recurrent neural network for classification.\n",
    "### Embedding\n",
    "Embedding means find a corresponding numerical vector for each word, which is now an\n",
    "integer in the list.\n",
    "The numerical vector can be seen as the coordinate of a point in the space.\n",
    "We embed the words into specific points in the space.\n",
    "That is why we call the process embedding.\n",
    "\n",
    "To implement it, we use a Keras Embedding layer.\n",
    "First, we need to create a Keras Sequential model.\n",
    "Then, we add the layers one by one to the model.\n",
    "The order of the layers is from the input to the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "max_words = 10000\n",
    "embedding_dim = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In the code above, we initialized an Embedding layer.\n",
    "The max_words is the vocabulary size, which is an integer meaning how many different\n",
    "words are there in the input data.\n",
    "The integer 16 means the length of the vector representation fro each word is 32.\n",
    "The output tensor of the Embedding layer is (batch_size, max_len, embedding_dim).\n",
    "### Recurrent Neural Networks\n",
    "After the embedding layer, we need to use a recurrent neural network for the\n",
    "classification.\n",
    "Recurrent neural networks can handle sequential inputs.\n",
    "For example, we input a movie review as a sequence of word embedding vectors to it, which\n",
    "are the output of the embedding layer.\n",
    "Each vector has a length of 32.\n",
    "Each review contains 100 vectors.\n",
    "If we see the RNN as a whole, it takes 100 vectors of length 16 altogether.\n",
    "However, in the real case, it takes one vector at a time.\n",
    "\n",
    "Each time the RNN takes in a word embedding vector,\n",
    "it not only takes the word embedding vector, but another state vector as well.\n",
    "You can think the state vector as the memory of the RNN.\n",
    "It memorizes the previous words it taken as input.\n",
    "In the first step, the RNN has no previous words to remember.\n",
    "It takes an initial state, which is usually empty,\n",
    "and the first word embedding vector as input.\n",
    "The output of the first step is actually the state to be input to the second step.\n",
    "For the rest of the steps, the RNN will just take the previous output and the current\n",
    "input as input,\n",
    "and output the state for the next step.\n",
    "For the last step, the output state is the final output we will use for the\n",
    "classification.\n",
    "\n",
    "We can use the following python code to illustrate the process.\n",
    "```python\n",
    "state = [0] * 32\n",
    "for i in range(100):\n",
    "    state = rnn(embedding[i], state)\n",
    "return state\n",
    "```\n",
    "The returned state is the final output of the RNN.\n",
    "\n",
    "Sometimes, we may also need to collect the output of each step as shown in the following\n",
    "code.\n",
    "```python\n",
    "state = [0] * 32\n",
    "output = []\n",
    "for i in range(100):\n",
    "    state = rnn(embedding[i], state)\n",
    "    output.append(state)\n",
    "return output\n",
    "```\n",
    "In the code above, the output of an RNN can also be a sequence of vectors, which is the\n",
    "same format as the input to the RNN.\n",
    "Therefore, we can make the RNN deeper by stacking multiple RNN layers together.\n",
    "\n",
    "\n",
    "To implement the RNN described, we need the SimpleRNN layer in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model.add(SimpleRNN(embedding_dim, return_sequences=True))\n",
    "model.add(SimpleRNN(embedding_dim, return_sequences=True))\n",
    "model.add(SimpleRNN(embedding_dim, return_sequences=True))\n",
    "model.add(SimpleRNN(embedding_dim))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The return_sequences parameter controlls whether to collect all the output vectors of an\n",
    "RNN or only collect the last output. It is set to False by default.\n",
    "### Classification Head\n",
    "Then we will use the output of the last SimpleRNN layer, which is a vector of length 32,\n",
    "as the input to the classification head.\n",
    "In the classification head, we use a fully-connected layer for the classification.\n",
    "Then we compile and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", metrics=[\"acc\"], loss=\"binary_crossentropy\")\n",
    "model.fit(train_data, train_labels, epochs=2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then we can validate our model on the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3.3-RNN",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}